---
title: Practical 2
date: today
date-format: long
author: Melvil Deleage, Jeff Macaraeg
toc: true
format:
  pdf:
    code-fold: false
    toc: true
    code-block-border-left: "#0d6efd"
    code-block-bg: true
    highlight-style: github
    geometry: margin = 1.5cm
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# load the required packages and install them if they are not.
source(here::here("setup.R"))
```

# Part 1 - Venice

The `venice90` dataset can be found in the `VGAM` package.

## Question a) 

> Read in the data. Extract and represent the yearly max values from 1940 to 2009. What do you observe ?

```{r}
library(VGAM)
data(venice90)

# Transform venice90 into a data frame
venice90_df <- as.data.frame(venice90)

# Group by year and extract the maximum sea level per year between 1940 to 2009
yearly_max <- venice90_df %>%
  group_by(year) %>%
  summarise(max_sealevel = max(sealevel))

# Plot the yearly maximum sea levels
ggplot(yearly_max, aes(x = year, y = max_sealevel)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color ="red", size =1)
  labs(x = "Year", y = "Maximum Sea Level (cm)",
       title = "Yearly Maximum Sea Levels in Venice (1940-2009)") +
  theme_minimal()
```

We can observe some variability over the years and a slight upward trend, so the maximum levels in Venice seem to be increasing.

## Question b)

> We are end of 2009 and would like to predict the yearly maximum values over the next 13 years (from 2010 to 2022). A naive approach consists of fitting a linear model on the observed yearly maxima and predict their values for 2010–2022. Proceed to this prediction and provide confidence intervals.

```{r}
# Fit linear model
model <- lm(max_sealevel ~ year, data = yearly_max)

# Predict for 2010–2022 with confidence intervals
future_years <- data.frame(year = 2010:2022)
pred <- predict(model, newdata = future_years, interval = "confidence", level = 0.95)

# Show predictions
cbind(future_years, pred)

# Combine predictions with years
pred_df <- cbind(future_years, as.data.frame(pred))

# Plot observed and predicted with confidence intervals
ggplot() +
  geom_line(data = yearly_max, aes(x = year, y = max_sealevel), color = "blue") +
  geom_point(data = yearly_max, aes(x = year, y = max_sealevel), color = "red", size = 1) +
  geom_line(data = pred_df, aes(x = year, y = fit), color = "darkgreen") +
  geom_ribbon(data = pred_df, aes(x = year, ymin = lwr, ymax = upr), fill = "lightgreen", alpha = 0.4) +
  labs(x = "Year", y = "Maximum Sea Level (cm)",
       title = "Observed and Predicted Yearly Maximum Sea Levels") +
  theme_minimal()
```

We used a confidence interval of 95% to predict for the years 2010 to 2022.

## Question c)

> Represent in the same graph the predicted yearly max values for the period 2010–2022, their pointwise confidence bounds and the observed values greater than 140 cm from the table below.

```{r}
# Observed values > 140 cm
extreme_vals <- yearly_max %>% filter(max_sealevel > 140)

# Plot everything together
ggplot() +
  # Historical data
  geom_line(data = yearly_max, aes(x = year, y = max_sealevel), color = "blue") +
  geom_point(data = yearly_max, aes(x = year, y = max_sealevel), color = "red", size = 1) +
  
  # Predictions
  geom_line(data = pred_df, aes(x = year, y = fit), color = "darkgreen") +
  geom_ribbon(data = pred_df, aes(x = year, ymin = lwr, ymax = upr),
              fill = "lightgreen", alpha = 0.4) +
  
  # Highlight points > 140 cm
  geom_point(data = extreme_vals, aes(x = year, y = max_sealevel),
             color = "black", size = 2, shape = 17) +  # triangle shape
  
  labs(x = "Year", y = "Maximum Sea Level (cm)",
       title = "Predicted Max Sea Levels (2010–2022) with Historical Extremes (>140 cm)") +
  theme_minimal()
```

This plot provides all the necessary information, from the historical data in the blue line, to the yearly maximum values with the red points, the dark green line being the prediction for 2010 to 2022, the light green area being the confidence intervals and finally, the black triangles being the values greater than 140cm.

> Now we perform a risk analysis and because we are interested in the period 2010–2022, we want to calculate the 13-years return level., for each year.

## Question d)

> Fit a GEV a with constant parameters to the historical yearly max values. Fit a GEV with time varying location parameter. Compare the two embedded models using likelihood ratio test (LRT). Show diagnostic plots.

```{r}
# Fit GEV with constant parameters
gev_const <- vglm(max_sealevel ~ 1, gev, data = yearly_max)

# Fit GEV with location changing over time (year)
gev_time <- vglm(max_sealevel ~ year, gev, data = yearly_max)

# Compare models
lrtest(gev_const, gev_time)

# Plot diagnostic for time-varying model
par(mfrow = c(2, 2))
plot(gev_time, which = 1:4)
```

The model looks overall okay, despite having some outliers which might influence it. There are no major pattern nor heteroskedasticity.

## Question e)

> Add if necessary a time varying scale and or shape GEV parameter. Select the best model according to LRT.

```{r}
# Fit base model
gev_loc <- vglm(max_sealevel ~ year, gev, data = yearly_max)

# Add time-varying scale
gev_loc_scale <- vglm(max_sealevel ~ year, gev, data = yearly_max, form2 = ~ year)

# Add time-varying shape
gev_loc_scale_shape <- vglm(max_sealevel ~ year, gev, data = yearly_max, form2 = ~ year, form3 = ~ year)

# Compare models
lrtest(gev_loc, gev_loc_scale)
lrtest(gev_loc_scale, gev_loc_scale_shape)
```

The LogLik values are identical, with the same Chisq and p-values. Thus, we don't have evidence that adding a time-varying scale or shape improves the model. So, the best model is the simplest one, the one with time-varying location only.

## Question f)

> Predict the 13-years return level, each year from 2010 to 2022.

```{r}
# Extract relevant coefficients
intercept <- -392.1969947   # (Intercept):1
beta <- 0.2568331           # year
sigma <- 2.5776601          # (Intercept):2

# Compute mu for 2010–2022
years <- 2010:2022
mu <- intercept + beta * years

# 13-year return level using Gumbel formula
T <- 13
return_level <- mu - sigma * log(-log(1 - 1/T))

# Final table
ret_level <- data.frame(year = years, return_level = return_level)
```

## Question g)

> Calculate confidence bands for these predictions.

```{r}
# Get standard errors se
summary(gev_loc)@coef3

# Standard errors (from summary)
intercept_se <- 153.70667449
beta_se <- 0.07784078

# Compute standard error of mu
se_mu <- sqrt(intercept_se^2 + (years^2) * beta_se^2)

# 95% confidence bands
lower <- return_level - 1.96 * se_mu
upper <- return_level + 1.96 * se_mu

# Final table
ret_level <- data.frame(year = years,
                        return_level = return_level,
                        lower_95 = lower,
                        upper_95 = upper)

# View the result
print(ret_level)
```

## Question h)

> Represent in the same graph your predictions of the 13-years return levels, their pointwise
confidence intervals, the predicted yearly max values from the linear model and the observed
values greater than 140 cm from the table below.

```{r}

```



# Part 2 - Nuclear Reactors

## Question a)

> Read in the data. Display a time series plot of the water level across the data range and try to
identify times of highest levels.

```{r}
# Load the Rdata file
load("data/Practical2/niveau.Rdata")

# Convert Zeitstempel to Date
niveau$Zeitstempel <- as.Date(niveau$Zeitstempel)

# Plot the time series
plot(niveau$Zeitstempel, niveau$Wert, type = "l", col = "blue",
     xlab = "Date", ylab = "Water Level (m ü.M.)",
     main = "Daily Maximum Water Level Over Time")

# Highlight top 5 highest water levels
top5 <- niveau[order(-niveau$Wert), ][1:5, ]
points(top5$Zeitstempel, top5$Wert, col = "red", pch = 19)
text(top5$Zeitstempel, top5$Wert, labels = round(top5$Wert, 2),
     pos = 3, cex = 0.8, col = "red")
```

## Question b)

> Now display a histogram of the water levels. What do you observe about the distribution?

```{r}
# Simple histogram of water levels
hist(niveau$Wert, 
     main = "Histogram of Water Levels", 
     xlab = "Water Level", 
     col = "skyblue", 
     border = "white")
```

The distribution is right-skewed. Most levels are concentrated between 325 and 326. Extreme levels such as above 327 are rare yet still present. These can represent potential flood events or unusual conditions.

> The FOEN plans for several degrees of risk. In this assignment, we focus on two risk levels: 50-year events and 100-year events.

## Question c)

> Explain how you would model the high water levels using a peaks-over-threshold approach.

```{r}
# Calculate the 95th percentile threshold
threshold <- quantile(niveau$Wert, 0.95)

# Extract exceedances above the threshold
exceedances <- niveau$Wert[niveau$Wert > threshold]

# Print threshold and number of exceedances
cat("95% threshold:", threshold, "\n")
cat("Number of exceedances:", length(exceedances), "\n")
```

Using a Peaks-over-Threshold approach, we set a threshold above which the values are considered extreme. This threshold should be high enough to focus only on rare exceedences, but not to high to avoid having too few exceedences. Here, the threshold is set at the 95th percentile, which is at 326.9117 (so around 329.91) meters. The exceedances are modeled using the Generalized Pareto Distribution, suitable for a skewed distribution. In this case, the POT approach is useful as there are a lot of non-extreme values. This approach thus focuses only on the extreme events to assess a better statistical efficiency, especially with daily data over many years.

## Question d)

> Comment on the aspect of clustering of extremes. How do you propose to measure and deal
with clustering of the daily water levels?

```{r}
# Set time gap for declustering (e.g., 3 days)
run_length <- 3

# Identify dates of exceedances
exceed_dates <- niveau$Zeitstempel[niveau$Wert > threshold]

# Sort dates
exceed_dates <- sort(exceed_dates)

# Initialize clusters
clusters <- list()
current_cluster <- c(exceed_dates[1])

for (i in 2:length(exceed_dates)) {
  if (as.numeric(exceed_dates[i] - tail(current_cluster, 1)) <= run_length) {
    current_cluster <- c(current_cluster, exceed_dates[i])
  } else {
    clusters <- append(clusters, list(current_cluster))
    current_cluster <- c(exceed_dates[i])
  }
}
clusters <- append(clusters, list(current_cluster))

# Extract cluster maxima
cluster_maxima <- sapply(clusters, function(cluster_dates) {
  max(niveau$Wert[niveau$Zeitstempel %in% cluster_dates])
})

# Results
cat("Number of exceedances before declustering:", length(exceedances), "\n")
cat("Number of cluster maxima (after declustering):", length(cluster_maxima), "\n")
```

Clustering extremes uses runs methods. We keep only one peak per cluster, which makes the exceedances more independent and suitable for modelling.

## Question e)

> Perform the analysis you suggest in c) and d) and compute the 50- and 100-year return levels.
Explain your choice of threshold and provide an estimate of uncertainty for the return levels.
Note: take care to compute the return level in yearly terms.

Using the POT approach:

```{r}
# Fit GPD model
fit <- gpd.fit(cluster_maxima, threshold)

# Basic info
sigma <- fit$mle[1]
xi <- fit$mle[2]
cov_mat <- fit$cov

# Estimate exceedance rate per year
years <- as.numeric(difftime(max(niveau$Zeitstempel), min(niveau$Zeitstempel), units = "days")) / 365.25
lambda <- length(cluster_maxima) / years

# Return level function
return_level <- function(T, sigma, xi) {
  threshold + (sigma / xi) * ((T * lambda)^xi - 1)
}

# Simulate 1000 sets of parameters
set.seed(1)
params <- mvrnorm(1000, mu = c(sigma, xi), Sigma = cov_mat)

# Calculate return levels
rl_50 <- apply(params, 1, function(p) return_level(50, p[1], p[2]))
rl_100 <- apply(params, 1, function(p) return_level(100, p[1], p[2]))

# Get point estimates
rl_50_est <- return_level(50, sigma, xi)
rl_100_est <- return_level(100, sigma, xi)

# Confidence intervals
ci_50 <- quantile(rl_50, c(0.025, 0.975))
ci_100 <- quantile(rl_100, c(0.025, 0.975))

# Print nicely
cat("50-year return level:", round(rl_50_est, 2), "\n")
cat("95% CI:", round(ci_50[1], 2), "-", round(ci_50[2], 2), "\n\n")

cat("100-year return level:", round(rl_100_est, 2), "\n")
cat("95% CI:", round(ci_100[1], 2), "-", round(ci_100[2], 2), "\n")
```

The threshold is the 95th percentile to capture the extremes, have a balance between bias and variance and to have an adequate sample size to fit a GPD

## Question f)

> Explain the drawbacks and advantages of using a block maxima method instead of the one used
in c)-e).

The Block Maxima method selects the maximum observation from a given time interval, but uses only one observation per block which leads to an inefficient use of the data. The POT approach uses all the values above the given threshold and handles clustering well via declustering. It is more efficient and flexible especially when extreme events happen in clusters. Thus, the POT approach is more precise and provides more information on the behavior of extreme events.

# Part 3 - Night temperatures in Lausanne

## Question a)

> Read in the data for the daily night maximum temperatures in Lausanne. Subset the summer
months (June to September).

```{r}
# Read the CSV file
nightmax <- read.csv("data/Practical2/nightmax.csv")

# Convert date column to Date format (assuming the column is named "date")
nightmax$date <- as.Date(nightmax$date)

# Extract month
nightmax$month <- month(nightmax$date)

# Subset for summer months (June = 6 to September = 9)
summer_nightmax <- subset(nightmax, month >= 6 & month <= 9)
```

## Question b)

> Assess whether extremes of the subsetted series in (a) occur in cluster.

```{r}
# Step 1: Subset summer months (June to September)
lausanne_data <- subset(nightmax, month >= 6 & month <= 9)

# Step 2: Set threshold at 95th percentile
threshold_laus <- quantile(lausanne_data$night.max, 0.95, na.rm = TRUE)

# Step 3: Create exceedance indicator
exceed_laus <- lausanne_data$night.max > threshold_laus

# Step 4: Run length encoding to find clusters
rle_exceed <- rle(exceed_laus)
cluster_lengths <- rle_exceed$lengths[rle_exceed$values == TRUE]

# Step 5: Compute extremal index
num_exceedances <- sum(exceed_laus, na.rm = TRUE)
num_clusters <- length(cluster_lengths)
extremal_index <- num_clusters / num_exceedances

# Output the extremal index
print(paste("Extremal Index (θ):", round(extremal_index, 3)))
```

The obtained extremal index is 0.564, which is lower than 1. This suggests that extreme night temperatures during summer in Lausanne tend to occur in clusters rather than being isolated. This means that if you observe one extremely hot night, there is a higher chance that other extreme nights will follow shortly, such as during a heatwave for example.

## Question c)

> Decluster the data from (a) using a suitable threshold. Plot the resulting declustered data.
(Hint: you may want to use the extRemes package.)

```{r}
# Extract and clean summer night max temperatures
summer_temps <- na.omit(lausanne_data$night.max)

# Set 95th percentile threshold
threshold <- quantile(summer_temps, 0.95)

# Decluster the series
declustered <- decluster(summer_temps, threshold = threshold)

# Line plot of the declustered extremes
plot(declustered, type = "l", col = "blue", lwd = 2,
     main = "Line Plot of Declustered Extreme Night Max Temperatures",
     xlab = "Index (declustered events)", ylab = "Temperature (°C)")
```

## Question d)

> Fit a GPD to the data, both raw and declustered. Assess the quality of the fit.

```{r}
# Fit GPD to raw data
gpd_raw <- fevd(summer_temps, threshold = threshold, type = "GP", method = "MLE")

# Fit GPD to declustered data
declustered <- decluster(summer_temps, threshold = threshold)
gpd_declust <- fevd(declustered, threshold = threshold, type = "GP", method = "MLE")
```

```{r}
plot(gpd_raw, main = "Raw Data - GPD Fit")
```

For the raw data, looking at the QQ-plot, the points seem quite aligned with the line, so the fit seems okay, same thing for the PP-plot and the return level plot. The density plot shows some imperfection, but the fit seems decent.

```{r}
plot(gpd_declust, main = "Declustered Data - GPD Fit")
```

For the declustered plot, the QQ-plot and return level seem decent despite some imperfections. However, the PP-plot seem to have some deviations and some points go outside the confidence bounds, indicating a weaker fit. For the density plot, there is a bit of a mismatch at the peak, though the tails are well captured. The declustered data seem to not fit as good as the raw one.

## Question e)

> Repeat the above analysis for the negatives of the daily nightly minimum temperatures for the winter months (November-February).

```{r}
# Read the CSV file
nightmin <- read.csv("data/Practical2/nightmin.csv")

# Convert date column to Date format
nightmin$date <- as.Date(nightmin$date)

# Extract month
nightmin$month <- month(nightmin$date)

# Subset for winter months (November to February)
winter_nightmin <- subset(nightmin, month %in% c(11, 12, 1, 2))
```

Here, we want to analyze cold extremes.

```{r}
# Apply negative to the temperatures for cold extremes
winter_nightmin$neg_night_min <- -winter_nightmin$night.min

# Remove missing values
winter_temps <- na.omit(winter_nightmin$neg_night_min)

# Set threshold at 95th percentile
threshold_winter <- quantile(winter_temps, 0.95)

# Create exceedance indicator
exceed_winter <- winter_temps > threshold_winter

# Run-length encoding to find clusters
rle_exceed_winter <- rle(exceed_winter)
cluster_lengths_winter <- rle_exceed_winter$lengths[rle_exceed_winter$values == TRUE]

# Compute extremal index
num_exceedances_winter <- sum(exceed_winter, na.rm = TRUE)
num_clusters_winter <- length(cluster_lengths_winter)
extremal_index_winter <- num_clusters_winter / num_exceedances_winter

print(paste("Extremal Index (θ):", round(extremal_index_winter, 3)))
```

We apply the negative to the winter values to treat the extremely low values as high values for modelling purposes. We then do an extremal index and we obtain 0.402, lower than 1, indicating clustering.

```{r}
# Decluster the series
declustered_wint <- decluster(winter_temps, threshold = threshold_winter)

# Plot line of declustered values
plot(declustered_wint, type = "l", col = "blue", lwd = 2,
     main = "Line Plot of Declustered Extreme Night Min Temperatures (Negated)",
     xlab = "Index (declustered events)", ylab = "Negated Temperature (°C)")
```

We then keep only one value per cluster.

```{r}
# Fit GPD to raw data
gpd_raw_wint <- fevd(winter_temps, threshold = threshold_winter, type = "GP", method = "MLE")

# Fit GPD to declustered data
gpd_declust_wint <- fevd(declustered, threshold = threshold_winter, type = "GP", method = "MLE")

plot(gpd_raw_wint, main = "Raw Data - GPD Fit (Negated Winter Min Temps)")
plot(gpd_declust_wint, main = "Declustered Data - GPD Fit (Negated Winter Min Temps)")
```

We fit a GPD to both raw and declustered data and assess the quality of the GPD fit for both data.
The GPD fit to the raw data shows a reasonably good performance compared to the decluster fit. The latter is due to the reduced number of data points after declustering.
